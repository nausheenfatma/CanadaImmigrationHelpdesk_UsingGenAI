{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thsi notebook shows how to write simple Prompt templates custom Agent tools using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Without Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "# Initialize the Ollama model (make sure it's running locally)\n",
    "llm = OllamaLLM(model =\"llama2\") \n",
    "\n",
    "# Wrap into a Runnable using a simple lambda or chain\n",
    "chain = RunnableLambda(lambda prompt: llm.invoke(prompt))\n",
    "\n",
    "# Invoke the model\n",
    "response = chain.invoke(\"What's the capital of Canada?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 'With' Prompt template and passable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "# Chain it\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke(\"Canada\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke(\"India\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"US\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Agent Tool with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Define the tool (manually)\n",
    "def get_squared_func(number: str) -> str:\n",
    "    \"\"\"Takes a number as a string and returns its square. Handles messy input.\"\"\"\n",
    "    # Extract the first integer from the string\n",
    "    import re\n",
    "    match = re.search(r'\\d+', number)\n",
    "    if not match:\n",
    "        return \"Invalid input: No number found\"\n",
    "    num = int(match.group())\n",
    "    return str(num ** 2)\n",
    "\n",
    "\n",
    "get_squared_tool = Tool(\n",
    "    name=\"get_squared\",\n",
    "    func=get_squared_func,\n",
    "    description=\"Takes an number string as input and returns its square.\"\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=[get_squared_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=2, \n",
    "     early_stopping_method=\"generate\"  # Optional: finish with best guess\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'225'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## # Now this prompt style is valid\n",
    "# agent.run(\"What is the square of 12?\")\n",
    "# directly calling the tool\n",
    "\n",
    "get_squared_tool.run('15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Agent tool as a Runnable pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12² = 144, 36² = 1296\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Step 1: Define the get_squared tool\n",
    "def get_squared(number: int) -> int:\n",
    "    return number * number\n",
    "\n",
    "# Step 2: Parse numbers from input\n",
    "def extract_numbers(text: str) -> list[int]:\n",
    "    return [int(n) for n in re.findall(r'\\d+', text)]\n",
    "\n",
    "# Step 3: Build a LangChain Runnable pipeline\n",
    "# - Input: user prompt\n",
    "# - Output: formatted result string\n",
    "pipeline = (\n",
    "    RunnableLambda(lambda text: extract_numbers(text))  # [\"12\", \"36\"]\n",
    "    | RunnableLambda(lambda nums: [(n, get_squared(n)) for n in nums])  # [(12, 144), (36, 1296)]\n",
    "    | RunnableLambda(lambda results: \", \".join(f\"{n}² = {sq}\" for n, sq in results))  # \"12² = 144, 36² = 1296\"\n",
    ")\n",
    "\n",
    "# Optional: use Ollama to add LLM reasoning before/after\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Run the pipeline\n",
    "user_input = \"What is the square of 12 and 36?\"\n",
    "result = pipeline.invoke(user_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15² = 225\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "user_input = \"What is the square of 15?\"\n",
    "result = pipeline.invoke(user_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the output using Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hey there! So, you know how we multiplied 15 by itself? Well, when you multiply a number by itself, the result is always going to be bigger than the original number! Isn't that cool? So, in this case, 15 multiplied by itself equals 225. That means that 15 squared is equal to 225! Pretty neat, right?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Add an LLM explanation step\n",
    "explanation_prompt = PromptTemplate.from_template(\n",
    "    \"Given these squared results: {results}, explain them in a friendly sentence.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    pipeline\n",
    "    | RunnableMap({\"results\": lambda x: x})  # name the result for the prompt\n",
    "    | explanation_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke(user_input))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canadaimmigrationhelpdesk-usinggenai-s0qUKeX3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
