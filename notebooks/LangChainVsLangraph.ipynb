{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain vs Langgraph\n",
    "\n",
    "LangChain and LangGraph are both libraries developed by LangChain Inc., but they serve different purposes within the ecosystem of building LLM (Large Language Model)-powered applications. Here's a clear breakdown of their differences:\n",
    "\n",
    "ðŸ”· LangChain\n",
    "What it is: A framework for building applications with LLMs using chains, agents, tools, memory, and retrieval.\n",
    "\n",
    "Primary abstraction: Chains (sequential steps of logic) and Agents (models that decide which tools to use at each step).\n",
    "\n",
    "Use cases: Q&A bots, document retrieval systems, code assistants, customer support agents, etc.\n",
    "\n",
    "Key components:\n",
    "\n",
    "Chains: Sequence of LLM calls.\n",
    "\n",
    "Agents: Dynamic decision-making using tools.\n",
    "\n",
    "Memory: For stateful conversations.\n",
    "\n",
    "Retrievers: For RAG (retrieval augmented generation).\n",
    "\n",
    "ðŸ”¶ LangGraph\n",
    "What it is: A stateful computation framework built on top of LangChain, using graphs instead of chains.\n",
    "\n",
    "Primary abstraction: State machines / Graphs â€” nodes are functions (often LLM calls), and edges represent conditional transitions.\n",
    "\n",
    "Use cases: More complex workflows with loops, branches, parallelism, and state management.\n",
    "\n",
    "Built on: NetworkX (Python graph library) + LangChain primitives.\n",
    "\n",
    "Key feature: Can model multi-step, multi-agent, or recursive interactions more naturally than LangChain alone.\n",
    "\n",
    "ðŸ§  Analogy:\n",
    "LangChain = Pipeline (linear, or dynamic with agents).\n",
    "\n",
    "LangGraph = Flowchart/State Machine (more control over flow, branching, and re-entry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case: Sentiment Analysis with Conditional Follow-Up\n",
    "Input: User enters a sentence.\n",
    "\n",
    "Step 1: Detect sentiment (positive/negative/neutral).\n",
    "\n",
    "Step 2:\n",
    "\n",
    "If positive: Reply with a cheerful response.\n",
    "\n",
    "If negative: Ask a follow-up question.\n",
    "\n",
    "If neutral: Thank the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Langchain approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that. Want to talk about it?\n"
     ]
    }
   ],
   "source": [
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "#llm = ChatOpenAI()\n",
    "llm = ChatOllama(model=\"llama2\")  # or \"mistral\", \"llama2\", etc.\n",
    "\n",
    "# Step 1: Sentiment detection\n",
    "sentiment_prompt = PromptTemplate.from_template(\"You are a sentiment analysis tool. What is the sentiment of this sentence: '{text}'? (positive, negative, neutral)\")\n",
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt)\n",
    "\n",
    "# Step 2: Define responses\n",
    "def handle_sentiment(text):\n",
    "    sentiment = sentiment_chain.run(text).strip().lower()\n",
    "\n",
    "    if sentiment == \"positive\":\n",
    "        return \"That's great to hear! ðŸ˜Š\"\n",
    "    elif sentiment == \"negative\":\n",
    "        return \"I'm sorry to hear that. Want to talk about it?\"\n",
    "    else:\n",
    "        return \"Thanks for sharing that!\"\n",
    "\n",
    "# Run it\n",
    "user_input = \"I had a terrible day at work.\"\n",
    "response = handle_sentiment(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for sharing that!\n"
     ]
    }
   ],
   "source": [
    "# Run it\n",
    "user_input = \"Its such a bright sunny day. I feel happy today.\"\n",
    "response = handle_sentiment(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for sharing that!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"It was a horrible iphone. It kept hanging. I found it very difficult to operate.\"\n",
    "response = handle_sentiment(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for sharing that!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"It was a great movie. I highly recommend. Lots of suspense\"\n",
    "response = handle_sentiment(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Langgraph approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: content='\\nNegative' additional_kwargs={} response_metadata={'model': 'llama2', 'created_at': '2025-05-06T22:20:04.958740157Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1096641820, 'load_duration': 6850113, 'prompt_eval_count': 54, 'prompt_eval_duration': 228039219, 'eval_count': 5, 'eval_duration': 861005683} id='run-dc0a88b4-eef9-474e-885b-73049a468b2e-0'\n",
      "I'm sorry to hear that. Want to talk about it?\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define schema\n",
    "class SentimentState(TypedDict):\n",
    "    input: str\n",
    "    sentiment: str\n",
    "    output: str\n",
    "\n",
    "#llm = ChatOpenAI()\n",
    "\n",
    "# Define nodes\n",
    "def detect_sentiment(state: SentimentState) -> dict:\n",
    "    text = state[\"input\"]\n",
    "    prompt = f\"What is the sentiment of this sentence: '{text}'? Answer only one word from these 3 options : positive, negative, or neutral\"\n",
    "    result = llm.invoke(prompt)\n",
    "    \n",
    "    print (\"result:\", result)\n",
    "    return {\"sentiment\": result.content.strip().lower()}\n",
    "\n",
    "def handle_positive(state: SentimentState) -> dict:\n",
    "    return {\"output\": \"That's great to hear! ðŸ˜Š\"}\n",
    "\n",
    "def handle_negative(state: SentimentState) -> dict:\n",
    "    return {\"output\": \"I'm sorry to hear that. Want to talk about it?\"}\n",
    "\n",
    "def handle_neutral(state: SentimentState) -> dict:\n",
    "    return {\"output\": \"Thanks for sharing that!\"}\n",
    "\n",
    "# Build the graph with state schema\n",
    "builder = StateGraph(SentimentState)\n",
    "\n",
    "builder.add_node(\"detect\", detect_sentiment)\n",
    "builder.add_node(\"positive\", handle_positive)\n",
    "builder.add_node(\"negative\", handle_negative)\n",
    "builder.add_node(\"neutral\", handle_neutral)\n",
    "\n",
    "builder.set_entry_point(\"detect\")\n",
    "\n",
    "# Conditional logic\n",
    "def route(state: SentimentState) -> str:\n",
    "    return state[\"sentiment\"]\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"detect\",\n",
    "    route,\n",
    "    {\n",
    "        \"positive\": \"positive\",\n",
    "        \"negative\": \"negative\",\n",
    "        \"neutral\": \"neutral\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Terminate after handling\n",
    "builder.add_edge(\"positive\", END)\n",
    "builder.add_edge(\"negative\", END)\n",
    "builder.add_edge(\"neutral\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# Run it\n",
    "result = graph.invoke({\"input\": \"I had a terrible day at work.\"})\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: content='\\nPositive' additional_kwargs={} response_metadata={'model': 'llama2', 'created_at': '2025-05-06T22:20:35.982336503Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3308394600, 'load_duration': 11149529, 'prompt_eval_count': 55, 'prompt_eval_duration': 2613716451, 'eval_count': 4, 'eval_duration': 682928844} id='run-48cffdbd-78cf-4198-907e-7c0aae491792-0'\n",
      "That's great to hear! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"input\": \"I had an amazing day at work.\"})\n",
    "print(result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canadaimmigrationhelpdesk-usinggenai-s0qUKeX3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
