{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook shows how to write simple Prompt templates, custom Agent tools using Langchain and how Routing works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Without Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of Canada is Ottawa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "\n",
    "# Initialize the Ollama model (make sure it's running locally)\n",
    "llm = OllamaLLM(model =\"llama2\") \n",
    "\n",
    "# Wrap into a Runnable using a simple lambda or chain\n",
    "chain = RunnableLambda(lambda prompt: llm.invoke(prompt))\n",
    "\n",
    "# Invoke the model\n",
    "response = chain.invoke(\"What's the capital of Canada?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 'With' Prompt template and passable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_787516/2285740442.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'Canada', 'text': '\\nThe capital of Canada is Ottawa.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "# Chain it\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke(\"Canada\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'India', 'text': '\\nThe capital of India is New Delhi.'}\n"
     ]
    }
   ],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke(\"India\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'US', 'text': '\\nThe capital of the United States is Washington, D.C. (District of Columbia).'}\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"US\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Agent Tool with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_787516/496600779.py:22: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Define the tool (manually)\n",
    "def get_squared_func(number: str) -> str:\n",
    "    \"\"\"Takes a number as a string and returns its square. Handles messy input.\"\"\"\n",
    "    # Extract the first integer from the string\n",
    "    import re\n",
    "    match = re.search(r'\\d+', number)\n",
    "    if not match:\n",
    "        return \"Invalid input: No number found\"\n",
    "    num = int(match.group())\n",
    "    return str(num ** 2)\n",
    "\n",
    "\n",
    "get_squared_tool = Tool(\n",
    "    name=\"get_squared\",\n",
    "    func=get_squared_func,\n",
    "    description=\"Takes an number string as input and returns its square.\"\n",
    ")\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_787516/1239551101.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=[get_squared_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=2, \n",
    "     early_stopping_method=\"generate\"  # Optional: finish with best guess\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'225'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## # Now this prompt style is valid\n",
    "# agent.run(\"What is the square of 12?\")\n",
    "# directly calling the tool\n",
    "\n",
    "get_squared_tool.run('15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Agent tool as a Runnable pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12Â² = 144, 36Â² = 1296\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Step 1: Define the get_squared tool\n",
    "def get_squared(number: int) -> int:\n",
    "    return number * number\n",
    "\n",
    "# Step 2: Parse numbers from input\n",
    "def extract_numbers(text: str) -> list[int]:\n",
    "    return [int(n) for n in re.findall(r'\\d+', text)]\n",
    "\n",
    "# Step 3: Build a LangChain Runnable pipeline\n",
    "# - Input: user prompt\n",
    "# - Output: formatted result string\n",
    "pipeline = (\n",
    "    RunnableLambda(lambda text: extract_numbers(text))  # [\"12\", \"36\"]\n",
    "    | RunnableLambda(lambda nums: [(n, get_squared(n)) for n in nums])  # [(12, 144), (36, 1296)]\n",
    "    | RunnableLambda(lambda results: \", \".join(f\"{n}Â² = {sq}\" for n, sq in results))  # \"12Â² = 144, 36Â² = 1296\"\n",
    ")\n",
    "\n",
    "# Optional: use Ollama to add LLM reasoning before/after\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Run the pipeline\n",
    "user_input = \"What is the square of 12 and 36?\"\n",
    "result = pipeline.invoke(user_input)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15Â² = 225\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "user_input = \"What is the square of 15?\"\n",
    "result = pipeline.invoke(user_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the output using Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hey there! So, you know how we've been learning about square numbers and stuff? Well, today we're gonna talk about something really cool - squares of numbers! That's right, I'm talking about when you take a number and multiply it by itself, like 15 x 15 = 225. It's like magic, right? You start with one number, and then you make it bigger and bigger just by multiplying it by itself! Isn't that awesome? ðŸ˜\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Add an LLM explanation step\n",
    "explanation_prompt = PromptTemplate.from_template(\n",
    "    \"Given these squared results: {results}, explain them in a friendly sentence.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    pipeline\n",
    "    | RunnableMap({\"results\": lambda x: x})  # name the result for the prompt\n",
    "    | explanation_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke(user_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example shows how to use 1 tool in a Langchain. Now we will explore how to use a router for LLM to pick from a bunch of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to implement a Router to pick tools: \n",
    "\n",
    "Below is a simple implementation of using two tools : Search and Calculator. And the LLM picks the tools based on the descriptions of the tool provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mA simple math problem! \n",
      "\n",
      "Thought: This one looks like a straightforward calculation.\n",
      "\n",
      "Action: CalculatorTool\n",
      "Action Input: \"42 * 19\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m798\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's my response:\n",
      "\n",
      "Question: What's 42 x 19?\n",
      "\n",
      "Thought: A simple math problem!\n",
      "\n",
      "Thought: This one looks like a straightforward calculation.\n",
      "\n",
      "Action: CalculatorTool\n",
      "Action Input: \"42 * 19\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m798\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI think I've got the answer already!\n",
      "\n",
      "Final Answer: The result of 42 x 19 is 798.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "âœ… Final Answer: The result of 42 x 19 is 798.\n"
     ]
    }
   ],
   "source": [
    "# --- Tools ---\n",
    "def search_tool_func(query: str) -> str:\n",
    "    print (\"Search tool was called\")\n",
    "    return f\"Searching for {query}\"\n",
    "\n",
    "search_tool = Tool.from_function(\n",
    "    func=search_tool_func,\n",
    "    name=\"SearchTool\",\n",
    "    description=\"Useful for when you need to search the web.\"\n",
    ")\n",
    "\n",
    "\n",
    "def calculator_tool_func(expression: str) -> str:\n",
    "    expression = expression.replace('x', '*').replace('X', '*')\n",
    "    expression = re.sub(r'[^\\d\\+\\-\\*/\\.\\(\\)]', '', expression)\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "calculator_tool = Tool.from_function(\n",
    "    func=calculator_tool_func,\n",
    "    name=\"CalculatorTool\",\n",
    "    description=\"Useful for solving math problems like '42 * 19'.\"\n",
    ")\n",
    "\n",
    "# --- LLM from Ollama ---\n",
    "llm = ChatOllama(model=\"llama3\")  # or \"mistral\", \"llama2\", etc.\n",
    "\n",
    "# --- Agent ---\n",
    "agent = initialize_agent(\n",
    "    tools=[calculator_tool, search_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=3 # <-- Stop after 3 steps\n",
    ")\n",
    "\n",
    "\n",
    "# --- Run It ---\n",
    "response = agent.run(\"What's 42 x 19?\")\n",
    "print(\"\\nâœ… Final Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Hmm, this seems like a non-math problem, so I won't be able to use the CalculatorTool. Let me think about what else I can do...\n",
      "\n",
      "Action: SearchTool\n",
      "Action Input: \"What is the capital of Canada?\"\u001b[0mSearch tool was called\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mSearching for What is the capital of Canada?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's my attempt:\n",
      "\n",
      "Question: What's the capital of Canada?\n",
      "\n",
      "Thought: Hmm, this seems like a non-math problem, so I won't be able to use the CalculatorTool. Let me think about what else I can do...\n",
      "\n",
      "Action: SearchTool\n",
      "Action Input: \"What is the capital of Canada?\"\u001b[0mSearch tool was called\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mSearching for What is the capital of Canada?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Ottawa!\n",
      "\n",
      "Here's my attempt:\n",
      "\n",
      "Question: What's the capital of Canada?\n",
      "\n",
      "Thought: Hmm, this seems like a non-math problem, so I won't be able to use the CalculatorTool. Let me think about what else I can do...\n",
      "\n",
      "Action: SearchTool\n",
      "Action Input: \"What is the capital of Canada?\"\u001b[0mSearch tool was called\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mSearching for What is the capital of Canada?\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "âœ… Final Answer: Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "# --- Run It ---\n",
    "response = agent.run(\"What's the capital of Canada?\")\n",
    "print(\"\\nâœ… Final Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can help you with that!\n",
      "\n",
      "Tokyo is in Japan Standard Time (JST), which is UTC+9. To get the current time in Tokyo, we need to add 13 hours to your Eastern Standard Time (EST) since there's a 5-hour difference.\n",
      "\n",
      "As I'm responding to you, it's:\n",
      "\n",
      "* 12:00 PM EST (your local time)\n",
      "* 1:00 AM JST (in Tokyo)\n",
      "\n",
      "Please note that Japan does not observe daylight saving time, so the UTC offset remains constant throughout the year.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I can help you with that!\n",
      "\n",
      "Tokyo is in Japan Standard Time (JST), which is UTC+9. To get the current time in Tokyo, we need to add 13 hours to your Eastern Standard Time (EST) since there's a 5-hour difference.\n",
      "\n",
      "As I'm responding to you, it's:\n",
      "\n",
      "* 12:00 PM EST (your local time)\n",
      "* 1:00 AM JST (in Tokyo)\n",
      "\n",
      "Please note that Japan does not observe daylight saving time, so the UTC offset remains constant throughout the year.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "#from langchain_openai import ChatOpenAI\n",
    "import datetime\n",
    "\n",
    "# 1. Define a custom tool (function)\n",
    "@tool\n",
    "def get_current_time(city: str) -> str:\n",
    "    \"\"\"Returns the current time in a given city.\"\"\"\n",
    "    # You can add actual timezone support with pytz/zoneinfo if needed\n",
    "    now = datetime.datetime.now()\n",
    "    return f\"The current time in {city} is {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# 2. Initialize the language model\n",
    "#llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "# --- LLM from Ollama ---\n",
    "llm = ChatOllama(model=\"llama3\")  # or \"mistral\", \"llama2\", etc.\n",
    "\n",
    "# 3. Initialize the agent with the tool\n",
    "agent = initialize_agent(\n",
    "    tools=[get_current_time],\n",
    "    llm=llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# 4. Run the agent\n",
    "response = agent.run(\"What is the exact current time in Tokyo? I am currently located in EST time zone\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding pydantic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canadaimmigrationhelpdesk-usinggenai-s0qUKeX3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
